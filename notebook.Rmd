---
title: "Zoe's Notebook"

output: html_document
editor_options: 
chunk_output_type: console
---

## Author: Zoe Portlas
### Affiliation: Plant Biology, UVM
### E-mail contact: zoe.portlas@uvm.edu


### Start Date: 2020-01-13
### End Date: 2020-05-08
### Project Descriptions:

# Table of Contents:  
  * [Entry 1: 2020-01-13, Monday](#id-section1)
  * [Entry 2: 2020-01-15, Wednesday](#id-section2)
  * [Entry 3: 2020-01-22, Monday](#id-section3)
  * [Entry 4: 2020-01-27, Monday](#id-section4)
  * [Entry 5: 2020-01-29, Wednesday](#id-section5)
  * [Entry 6: 2020-02-03, Monday](#id-section6)
  * [Entry 7: 2020-02-05, Wednesday](#id-section7)
  * [Entry 8: 2020-02-10, Monday](#id-section8)
  * [Entry 9: 2020-02-12, Wednesday](#id-section9)
  * [Entry 10: 2020-02-17, Monday](#id-section10)
  * [Entry 11: 2020-02-19, Wednesday](#id-section11)
  * [Entry 12: 2020-02-24, Monday](#id-section12)
  * [Entry 13: 2020-02-26, Wednesday](#id-section13)
  * [Entry 14: 2020-03-02, Monday](#id-section14)
  * [Entry 15: 2020-03-4, Wednesday](#id-section15)
  * [Entry 16: 2020-03-16, Monday](#id-section16)
  * [Entry 17: 2020-03-28, Wednesday](#id-section17)
  * [Entry 18: 2020-03-23, Monday](#id-section18)
  * [Entry 19: 2020-03-25, Wednesday](#id-section19)
  * [Entry 20: 2020-03-30, Monday](#id-section20)
       

------
<div id='id-section1'/>   

### Entry 1: 2020-01-13, Monday   

* Population genetics: 1920s - 1930s
  * Pioneers: Sewell Wright, R.A. Fisher, J.B.S. Haldane
  * Mathematical Theory
  * Classic expectations
    * Null model of Hardy Weinberg equilibrium
  * Genetic drift, sampling effect in finite populations
  * Island model, multiple local populations that were divided, but connected to each other through migration
    * Fst: fixation coefficiant of a sunpopulation in relation to the total population
      * Fst = 0 indicates complete panmixis, no differentiation between subpopulations
      * Fst = 1 indicates complete isolation of subpopulations
  * 1970s: allozymes, variants of the same protein to look at frequency of alleles
  * 1980s: PCR, studying DNA more directly, microsatellites, DNA Sanger Sequencing

* Molecular Evolution
  * Pioneers: M. Kimura (Neutral theory of evolution), Tajima (Tajima's D, ration of nucleotide diversity, pi, vs. number of segregating sites, s)
  * Tying in ecological data with next gen sequencing data
  

------
<div id='id-section2'/>   

### Entry 2: 2020-01-15, Wednesday   

#### Library Prep Info Updates

* RNA Seq
  * Whole transcriptome shotgun sequencing
  * Uses high thoroughout NGS (Next Gen Seq) to characterize gene expression patterns and to identify novel genes or transcripts
  * Platforms:
    * Solid
    * Ion Torrent
    * Illumina HiSeq
  * Expression at different time points, developmental stages, environment, across populations
  * Gives you functionally relevant information about what genes are actually being used
  * Can let you detect allele variants of those genes
  * Limitations:
    * Sometimes reads can be ambiguously mapped to the reference
    * Instability of mRNA
    * Can miss variation if you don't incorporate different tissues
  * Library Prep:
    * Sample collection (tissues, whole organisms)
    * RNA isolation and quality check (nanodrop, electrophoresis, qubit, bioanalyzer)
    * Ribosomal RNA depletion (rRNA) to focus on mRNA and polyA enrichment
    * Fragmentation to cut up mRNA into fragments that are the right size for sequencing platforms
    * Convert to cDNA (complementary DNA synthesis) reverse transcriptase
    * Add adaptors that may have amplification element, primary seq site, barcode, etc.
    * Amplification with PCR

* GBS
  * RADseq = Restriction site Associate DNA sequencing
    * A set of methods that start with genomic DNA and begin by digesting it with one or more restriction enzymes.
    * Adaptors (double stranded oligonucleotides) are also added for next gen sequencing.
    * Adaptors include barcodes which can be used to identify individual samples with allows multiplexing (pooling) so many samples can be sequenced as part of one library
  * Library Prep:
    * Extract DNA from samples
    * Digest DNA with one restriction enzymes (usually ApeKI), which leaves sticky ends
    * Ligate adaptors (with barcodes)
    * Multiplex
    * PCR
      * Indirect size selection for short fragments
      * Can introduce error because an allele might have a SNP at the site where the RE would usually cut, causing allele drop out

* Bisulfite Seq
  * Identifies methylated cytosine, which occur in CG islands
    * methyl added to C makes it hard for protein to interact
    * Isolate genomic DNA and treat with sodium bisulfite
    * Converts only methylated cytosine to uracil
    * PCR
    * Sequence and compare treated sample to the untreated sequence
  * Histones can be methylated or ethylated (etc.)
    * CHIPseq = Chromatin immunoprecipitation
    * Introduce a fixative to fix all protein to the DNA it's in contact with
    * Pink specific proteins that you're interested in with immunoprecipitation specificity system
      * Randomly fragment
      * Introduce an antibody
      * Antibody is attached to a pach that weighs down the protein/DNA it's attached to, to separate the protein/DNA of interest from others
    * Can sequence which parts of the DNA the protein was attached to
  * Chromatin Structure
    * ATACseq
    * Introduced manufactured transposon to fragment DNA where it's not wrapped in the chromatin
    * Transposonsattach adaptors so you can sequence
    * Fragmentation + tagging = Tagmentation
    * HiC seq
    
* Illumina Sequencing
  * Library Prep of choice
  * Cluster generation
    * Flow cell with lawn of complementary adaptors
    * Pieces of DNA with adaptors stick to lawn
    * Bridge PCR process amplifies DNA into clusters so the signal will be bright enough
  * Sequencing by synthesis
    * Fluorescent dNTPs
    * Signal of fluorescence recorded as DNA grows
  * Types: miSeq, HiSeq, Sanger
  
* PacBio/Oxford Nanopore
  * Long read sequencers
  * SMRT
    * PacBio
      * Polumerase is fixed in a well and the DNA moves through it
      * we know where the site of incorporation is going to be so you can fix a laser there
      * Fluorescent signal is read at that fixed point
      * To reduce error, they make sequence circular so it goes through multiple times
    * ONT
      * Nanopore
      * Combination of proteins: motor and one that sits in membrane
      * Single strand of DNA goes through pore
      * Current across membrane
    * Synthetic: Uses short reads with barcoding to stitch them together after
      * Illumina
      * 10x genomics (Immersion)
      * They partition longer fragments until they're in the lower frequency and then chop them up and give them barcodes
    * Inaccurate and expensive compared to short read sequencing
    * Good for highly repetitive regions
    
------
<div id='id-section3'/>   

### Entry 3: 2020-01-22, Monday   

##### Library Prep Info Updates Part 2
* Amplicon Seq
  * Amplicon: a piece of DNA/RNA that is product or source of amplification
    * Undergoes simplification, replicated genetic material
    * Direct repeat (head to tail)
    * Inverted repeat (head to head or tail to tail)
    * Amplicon usually interchangeable with term PCR product
    * PCR -> pooled -> sequenced together
    * Multiplexing
    * Can use for:
      * Genome targeting
      * Detection of hotspot mutations
      * Detection of gene fusion
      * SNPs
      * Metagenomics (biodiversity assessment)
    * Preparation
      * Sample selection (environmental samples, 'mock communities', etc
      * DNA/RNA extraction
      * Quantify using Qubit, etc.
      * ITS (fungal), CO1 (insects),  16SRNA (bacterial, ~450 bp)
      * Library prep: 
        * Samples
        * 1st PCR step
        * Add primer and head target (allows barcode to attach)
        * Purification
        * 2nd PCR: amplify your PCR products with primers that have library specific barcode
        * Purification (can use magnetic beads, magnetic stand, etc)
        * Pool samples into library that is sent to be sequenced
      * Why use 2 PCR steps? Only 1 leads to problems which can be remediated

* Exon capture sequencing
  * Focused on high value genomic regions so you can have high coverage sequences of prespecified regions
  * Exon = protein coding region of the gene
  * Exome = the portion of the genome that encodes for exons
  * Biomedical uses: disease variants most likely in exome
  * Ecological evolution: genetic mapping of locally adapted traits, fine scale mapping for phenotypic expression, can detect selection in genome, provides info of life history and molecular evolution, phylogenetic reconstruction, ancient DNA (aDNA)
  * Library prep
    * Sample collection
    * Fragmentation of the DNA
      * Fragment soup of DNA fragments with blunt ends, fragments same length, adenine tails are added, same adaptor is added to both ends of the fragments
    * Enriching library
      * Pool DNA that corresponds to exons, must create probes to capture DNA fragments with different methods:
        * PCR: range of targeted loci ~100, genomic knowledge needed of PCR primer sequences
        * De novo: range of targeted loci 100s-1000s, knowledge needed of transcriptome, RADseq, or whole genome sequences
        * Divergent annotated genome: 100s-1000s loci, knowledge needed of annotated genome
    * Purify and collect targeted DNA
    
------
<div id='id-section4'/>   

### Entry 4: 2020-01-27, Monday.   

#### Population Genomics
* Selection and drift across the genome
  * Motivation? 
    * Climate change -> change in abundance and distribution of species, and changes in selection
  * Challenging: signals of abundance/distribution (demographic history) are very difficult to tell from selection
  * Why study across the whole genome?
    * Inherently variable environment
    * Studying across the genome prevents bias from sampling only a few places along the genome
    * Linkage disequilibrium
    * Selection acts locally and demographic changes affect everywhere in the genome
  * Demographic history
    * Ne = effective population size, the size that a population behaves as if it were an idealized wright fisher population
      * Captures the effect of population size on drift
    * Migration (gene flow) among populations
    * Coalescent theory (Kingman, Watterson, Tajima, Hudson)
      * What is the probability that any two randomly sampled allele copies came from the same parent allele the generation before
      * 2Ne allele copies in a population of size Ne ind.
      * Pr(common ancestor) = 1/2Ne
      * Eventually coalesces to the common ancestor of the entire sample
      * Want to sample many genes, it's a stochastic process
      * Can estimate what pop size was doing based on how often you're seeing coalescent events
    * Site Frequency Spectrum (SFS)
      * Wright fisher population has many rare mutations
      * If pop is growing, there will be a lot of rare mutations and few common alleles
      * If a pop that has gone through a bottleneck, what you’re likely to lose is the rare lineages and the ones that make it through weren't rare before the bottleneck
  * Selection
    * Positive selection - selection increasing frequency of a beneficial allele
      * Pi = nucleotide diversity
      * Pi over chromosome, pi will decrease where there is selection because one allele is favored
      * With linkage disequilibrium, sites around the selected sites will also have decreased pi and there will be high linkage disequilibrium -> hitchhiking
      * Bushy topology = just recovering after a selective sweep
      * Same signature as population expansion
      * Tajima's D will be very negative for selective sweep and growing population

#### Bi, et al. 2019 



------
<div id='id-section5'/>   

### Entry 5: 2020-01-29, Wednesday   
(Absent from class)

* Difference between Rmd vs. md?
  * Markdown is a type of language that renders it into something readable
  
* Learning Objectives
  * Background on species
  * Understand the pipeline
  * Bash scripts
  * Sequencing quality assessment
  * Mapping to ref genome
  
* Red Spruce: Cool moist climates, northerly distributed, higher in elevation as you move south
  * Genetic basis of climate adaptation
  * Highly fragmented populations
  * Alleles that might be potentially conserved
  * Inform nature conservancy to improve restoration
  
* Genomic DNA from needles (110 Mother trees, 23 pops)
  * Red genome based on multiple tissues and developmental stages in related species white spruce
  * 80,000 proves were designed based on these transcriptomes (120 bp)
  * coverage of at least one prove for 38,570 unigenes (90 bp, 85% coverage)
  * fragmentation, hybridization with probes, sequenced on a Hiseq X to generate paired end 150 bp reads
    * single end only one end is paired
    * Paired ends two ends are sequenced, more powerful than single end

#### Pipeline
1. Visualize, Clean, Visualize
  * Visualize the quality of raw data (Program: FastQC)
  * Clean raw data (Program: Trimmomatic)
  * Visualize the quality of cleaned data (Program: FastQC)
2. Calculate #s of cleaned, high quality reads going into mapping
3. Map (a.k.a. Align) cleaned reads from each sample to the reference assembly to generate sequence alignment files (Program: bwa, Input: .fastq, Output: .sam)
4. Remove PCR duplicates identified during mapping and calculate alignment statistics (% of read mapping successfully, mapping quality scores, average depth of coverage per individual)

We'll then use the results of out mapping next week to start estimating diversity and population structure.


------
<div id='id-section6'/>   

### Entry 6: 2020-02-03, Monday.   

#### Settepani et al., 2017 
* Evolution of sociality in spiders leads to depleted genomic diversity at both population and species level

* Effective population size
  * Ne = 4(number males*number females)/(n males + n females)
  * Common in social pops to have unequal sex ratio with fewer males, so the Ne will be limited by the number of reproducing males


* Metapopulation dynamics
* Propagule pool model

------
<div id='id-section7'/>   

### Entry 7: 2020-02-05, Wednesday   

#### My population is DG

Qs from last week:
* paired reads are the ones we want to visualize because unpaired ones don't have partners that passed quality score. Paired useful for genome assembly, etc.

Learning Objectives

1. Reviewing our progress on read cleaning and visualizing QC
2. start mapping each set of cleaned reads to a reference genome
3. visualize seq alignment files
4. process our sam files by
  * converting to binary (bam) format and sorting by coordinates
  * removing PCR duplicates
  * indexing for fast future lookup
5. calculate mapping stats to assess quality of the result
6. learn how to put separate bash scripts into a "wrapper" that runs them all

* Mapping cleaned and trimmed reads against the ref genome
  * reference genome from Norway spruce (congenie.org)
  * pull out contigs from ref that contained one or more of our probes -> 668,000,000 bp reduced reference genome
  * genome isn't assembled into chromosomes, they're in contigs that are in some order (we don't know, which is typical for non-model organisms)
  * Indexed, reduced ref genome to use for mapping is on server here:
  `/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa`
  * N50 = gives a metric of the state of the assembly, how much of the genome is assembled into big contigs instead of small ones
    * sort contigs in order of size
    * take 50% of size (334 Mbp), start with biggest contigs and go until you have summed up that many Mbps
    * N50 is size of smallest contig that which the sum of contigs that size and larger equal 50% of the total size
    * you want it to be a big number to tell you at least half is assembled into big contigs
    * 101,375 bp is the N50 size for us
    * small is ok, but bigger gets you more spatial information, closer to the actual chromosomes
* We're going to write script in text editor
* test it for just one ind before running for all
* can kill scripts with ctrl+C


* on server: `git pull` to get scripts we wrote today
* to make file executable (if you do `ll` and the file names aren't green): `chmod u+x *.sh`

* at command prompt with nothing else running: `screen` and then `bash mypipeline.sh` and then ctrl+A and then ctrl+D `screen -r` (remember to detach again with ctrl+A and ctrl+D)

#### mypipeline.sh:

```
#!/bin/bash

# Path to my repo:

myrepo="/users/s/r/zportlas/Ecological_Genomics/Spring_2020"

# echo ${myrepo}

# My population:

mypop="DG"

# Directory to cleaned and paired reads:

input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"

# Directory in common space where we'll store mapping output for everyone to use

output="/data/project_data/RS_ExomeSeq/mapping"

# Run mapping.sh

source ./mapping.sh

# Run the post-processing steps

source ./process_bam.sh
```

#### mapping.sh:

```
#!/bin/bash

# this script will run the read mapping using "bwa" program

ref="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

# Write a loop to map each individual within my population

for forward in ${input}*_R1.cl.pd.fq

do
reverse=${forward/_R1.cl.pd.fq/_R2.cl.pd.fq}
f=${forward/_R1.cl.pd.fq/}
name=`basename ${f}`
bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam
done
```

#### process_bam.sh:

```
#!/bin/bash

# This is where our output sam files are going to get converted into binary (bam)
# Then we're going to sort the bam files, remove the PCR duplicates, and index them

# First, let's convert sam to bam and then sort

for f in ${output}/BWA/${mypop}*.sam

do
out=${f/.sam/}
sambamba-0.7.1-linux-static view -S --format=bam ${f} -o ${out}.bam
samtools sort ${out}.bam -o ${out}.sorted.bam
done

# Now lets remove the PCR duplicates from our bam files:
for file in ${output}/BWA/${mypop}*.sorted.bam

do
f=${file/.sorted.bam/}
sambamba-0.7.1-linux-static markdup -r -t 1 ${file} ${f}.sorted.rmdup.bam
done

# Now to finish, we'll index our files

for file in ${output}/BWA/${mypop}*.sorted.rmdup.bam

do
samtools index ${file}
done
```

------

------
<div id='id-section8'/>   

### Entry 8: 2020-02-10, Monday   

#### Milesi et al., 2019 
Assessing the potential for assisted gene flow using past introduction of Norway Spruce in Southern Sweden: Local adaptation and genetic basis of quantitative traits

------

------
<div id='id-section9'/>   

### Entry 9: 2020-02-12, Wednesday   
* ANGSD 
* Sam files are human readable but less data efficient than bam files
* MAQ = mapping quality score

* Filename sorted.rmdup.bam
* Bai companion file

* Genotype likelihood framework instead of hard calling genotypes because our read depth is low and we don't want to discount genotypes with lower probability
  * Genotype free pop gen


* Pairwise diffs (pi), segregating sites (theta_W), Tajima's D
* D = pi-theta_W/sd(pi-theta_W)
* D = 0 wright fisher
* D < 0 growing population
* D>0 bottlenecked population

#### Bamstats.sh:
```
#!/bin/bash

# set repo

myrepo="/users/z/p/zportlas/Ecological-Genomics"

mypop="DG"

output="/data/project_data/RS_ExomeSeq/mapping"

echo "Num.reads R1 R2 Paired MateMapped Singletons MateMappedDiffChr" >${myrepo}/myresults/${mypop}.flagstats.txt

for file in ${output}/BWA/${mypop}*sorted.rmdup.bam

do
f=${file/sorted.rmdup.bam/}
name=`basename ${f}`
echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt
samtools flagstat ${file} | awk 'NR>=6&&NR<=12 {print $1}' | column -x
done >> ${myrepo}/myresults/${mypop}.flagstats.txt

# calculate depth of coverage from our bam files

for file in ${output}/BWA/${mypop}*sorted.rmdup.bam

do
samtools depth ${file} | awk '{sum+=$3} END {print sum/NR}'
done >> ${myrepo}/myresults/${mypop}.coverage.txt
```

#### ANSGD_mypop.sh:
```
#!/bin/bash

myrepo="/users/z/p/zportlas/Ecological-Genomics"

mkdir ${myrepo}/myresults/ANGSD

output="${myrepo}/myresults/ANGSD"

mypop="DG"

ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}_*sorted.rm*.bam >${output}/${mypop}_bam.list

REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

# Estimating GL's and allele frequencies for all sites with ANGSD

ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-doHWE 1 \
# -SNP_pval 1e-6
```

------

------
<div id='id-section10'/>   

### Entry 10: 2020-02-17, Monday   


------

------
<div id='id-section11'/>   

### Entry 11: 2020-02-19, Wednesday   


------

------
<div id='id-section12'/>   

### Entry 12: 2020-02-24, Monday   


------

------
<div id='id-section13'/>   

### Entry 13: 2020-02-26, Wednesday   

Experimental Design:

* two source climates: 
  * wet/cool
  * hot/dry
* 10 maternal families (5 per source climate)
* 3 experimental treatments
  * Control: 23C/17C 16:8 L/D, daily water
  * Hot: 35C/26C, daily water
  * Hot+Dry: 35C/26C, no water
* 3 times:
  * harvested whole seedlings on days 0, 5, and 10
  * wanted 5 reps but some have less for day 5

Pipeline:

* FastQC (do pops NOR C and D)
* quant.sf files generated by Salmon -> input into DESeq2 for analysis and visualization

Questions:

* Do different individuals differ in gene expression across treatment
* expression at different time points
  * expression ~ Time + SourceClim + Treatment + Time*SourceClim + family
  * expression ~ Time + SourceClim + Treatment + SourceClim*Treatment +family 
  * expression ~ Time + SourceClim + Treatment + SourceClimxTreatment xTime +family 
  
Note: Trimmomatic already done, here is script:

```
#!/bin/bash

cd /data/project_data/RS_RNASeq/fastq/

########## Trimmomatic for single end reads

for R1 in *R1.fastq.gz  

do 
    echo "starting sample ${R1}"
    f=${R1/_R1.fastq.gz/}
    name=`basename ${f}`

    java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticSE \
        -threads 1 \
        -phred33 \
         "$R1" \
         /data/project_data/RS_RNASeq/fastq/cleanreads/${name}_R1.cl.fq \
        ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-SE.fa:2:30:10 \
        LEADING:20 \
        TRAILING:20 \
        SLIDINGWINDOW:6:20 \
        HEADCROP:12 \
        MINLEN:35 
     echo "sample ${R1} done"
done 
```
  
------

------
<div id='id-section14'/>   

### Entry 14: 2020-03-2, Monday   

Differential Expression

* differences in transcript abundance between pops or exp groups
* genotype to phenotype
* how: counting RNA with Salmon

* Normalization fixes problems with depth and library size
* DESeq2 accounts for depth and composition
  * log transforms abundance values and averages per gene,takes ratio of log transformed vals and uses median to calc scaling factor for each sample
  * divide each gene in sample by scaling factor

Challenges:

* false positives in large datasets
* false negatives, might have real overlapping distributions but only sample extreme edge and analyze them as different or vice versa
* overcome with independent filtering
  * DESeq2: runs a glm with neg binomial dist followed by Wald'd test to get adj. p vals

Vizualization:

* normalization plots, sequening summ stats
* barplots, of total read counts across samples
* number of null genes across samples
* normalized counts vs. samples
* PCA clustering 
* Heatmap: genes (rows) and samples (columns)
* Hierarchical clustering
* Venn diagrams
* MA Plot
* Volcano plot
* gene-trt response curves 
------

------
<div id='id-section15'/>   

### Entry 15: 2020-03-4, Wednesday   

Mapping populations to reference transcriptome

Salmon to quantify transcript abundance

base script:

```
salmon quant -i transcripts_index -l <LIBTYPE> -r reads.fq --validateMappings -o transcripts_quant
```

see `salmon.sh` in myscripts

------

------
<div id='id-section16'/>   

### Entry 16: 2020-03-16, Monday   
(class cancelled)

------

------
<div id='id-section17'/>   

### Entry 17: 2020-03-18, Wednesday   

* Many of our reads were not mapping because the reference we had selected included only the coding region
* When working with 3' RNAseq data, much of the data is likely to be in the untranslated region, so this region + coding region was combined to create a reference transcriptome with mapping rate of 52%

Read in counts matrix

```
library(tximportData)
library(tximport)
library(DESeq2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(readr)
library(wesanderson)
library(vsn)

countsTable <- read.table("RS_cds2kb_countsMatrix.txt", header=TRUE, row.names=1)

head(countsTable)
dim(countsTable)

countsTableRound <- round(countsTable) # Need to round because DESeq wants only integers
head(countsTableRound)

## Import the samples description table - links each sample to factors of the experimental design.

# Need the colClasses otherwise imports "day" as numeric which DESeq doesn't like, coula altneratively change to d0, d5, d10

conds <- read.delim("RS_samples.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1, colClasses=c('factor', 'factor', 'factor', 'factor'))

head(conds)
dim(conds)


## Let's see how many reads we have from each sample:

colSums(countsTableRound)
mean(colSums(countsTableRound))
barplot(colSums(countsTableRound), las=3, cex.names=0.5,names.arg = substring(colnames(countsTableRound),1,13))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd =2)

# what's the average num of counts per gene

rowSums(countsTableRound)
mean(rowSums(countsTableRound))
median(rowSums(countsTableRound))

# shows dispersion across genes, differences in magnitude of expression

apply(countsTableRound,2,mean)

## Create a DESeq object and define the experimental design here with the tilde

dds <- DESeqDataSetFromMatrix(
  countData = countsTableRound, 
  colData = conds,
  design = ~ climate + day + treatment)
dim(dds)
# [1] 66408    76

# Filter out genes with few reads

dds <-dds[rowSums(counts(dds)) > 76]
dim(dds)
# [1] 23887    76
# filtering to a sum of 76 read across all samples

## Run the DESeq model to test for differential gene expression:

# 1) estimate size factors (per sample), 
# 2) estimate dispersion (per gene), 
# 3) run negative binomial glm
dds <- DESeq(dds)

# List the results you've generated
resultsNames(dds)

# running model: design ~ pop+day+treatment
# [1] "Intercept"            "pop_BRU_05_vs_ASC_06"
# [3] "pop_CAM_02_vs_ASC_06" "pop_ESC_01_vs_ASC_06"
# [5] "pop_JAY_02_vs_ASC_06" "pop_KAN_04_vs_ASC_06"
# [7] "pop_LOL_02_vs_ASC_06" "pop_MMF_13_vs_ASC_06"
# [9] "pop_NOR_02_vs_ASC_06" "pop_XBM_07_vs_ASC_06"
# [11] "day_10_vs_0"          "day_5_vs_0"          
# [13] "treatment_D_vs_C"     "treatment_H_vs_C" 

# running model: design ~ climate+day+treatment
# [1] "Intercept"        "climate_HD_vs_CW"
# [3] "day_10_vs_0"      "day_5_vs_0"      
# [5] "treatment_D_vs_C" "treatment_H_vs_C"

# Order and list and summarize results from specific contrasts
res <- results(dds, alpha = 0.05)
res <- res[order(res$padj),]
head(res)

summary(res)
# out of 23887 with nonzero total read count
# adjusted p-value < 0.05
# LFC > 0 (up)       : 16, 0.067% (higher in hot, lower in control)
# LFC < 0 (down)     : 3, 0.013% (higher in control, lower in hot)
# outliers [1]       : 61, 0.26%
# low counts [2]     : 14300, 60%

res_treatCD <- results(dds, name="treatment_D_vs_C", alpha = 0.05)
res_treatCD <- res_treatCD[order(res_treatCD$padj),]
head(res_treatCD)

summary(res_treatCD)
# out of 23887 with nonzero total read count
# adjusted p-value < 0.05
# LFC > 0 (up)       : 678, 2.8%
# LFC < 0 (down)     : 424, 1.8%
# outliers [1]       : 61, 0.26%
# low counts [2]     : 7367, 31%

# Here you set your adjusted p-value cutoff, can make summary tables of the number of genes differentially expressed (up- or down-regulated) for each contrast


##### Data visualization #####
# MA plot
plotMA(res_treatCD, ylim = c(-3,3))
# red is transcripts that are sig expressed
# range of expression is pretty low, median of ~24 makes sense

# PCA
vsd <- vst(dds,blind = F)

data <- plotPCA(vsd, intgroup = c("climate","treatment", "day"), returnData = T)
percentVar <- round(100 * attr(data, "percentVar"))


data$treatment <- factor(data$treatment, levels=c("C","H","D"), labels = c("C","H","D"))

data$day <- factor(data$day, levels=c("0","5","10"), labels = c("0","5","10"))

ggplot(data, aes(PC1, PC2, color=day, shape=treatment)) +
  geom_point(size=4, alpha=0.85) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  theme_minimal()

# Counts of specific top gene! (important validatition that the normalization, model is working)

d <-plotCounts(dds, gene="MA_10426407g0030", intgroup = (c("treatment","climate")), returnData=TRUE)
d



p <-ggplot(d, aes(x=climate, y=count, shape=climate, colour = treatment)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.3,h=0), size=3) +
  scale_x_discrete(limits=c("CW","HD"))
p


d <-plotCounts(dds, gene="MA_10257300g0010", intgroup = (c("treatment","climate","day")), returnData=TRUE)
d



p <-ggplot(d, aes(x=treatment, y=count, color=day, shape=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.3,h=0), size=3) +
  scale_x_discrete(limits=c("C","H","D"))
p
# Heatmap of top 20 genes sorted by pvalue
library(pheatmap)
topgenes <- head(rownames(res_treatCD),20)
mat <- assay(vsd)[topgenes,]
mat <- mat - rowMeans(mat)
df <- as.data.frame(colData(dds)[,c("treatment","climate")])
pheatmap(mat, annotation_col=df)
```


------

------
<div id='id-section18'/>   

### Entry 18: 2020-03-23, Monday   


------

------
<div id='id-section19'/>   

### Entry 19: 2020-03-25, Wednesday   


------

------
<div id='id-section20'/>   

### Entry 20: 2020-03-30, Monday   


------